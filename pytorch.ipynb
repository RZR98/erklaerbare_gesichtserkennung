{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f832fa",
   "metadata": {},
   "source": [
    "Initiation of libraries and the mtcnn face detection pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c32dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob, random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import sklearn\n",
    "\n",
    "print(\"Setting up detection pipeline.\\n\")\n",
    "\n",
    "# If required, create a face detection pipeline using MTCNN:\n",
    "mtcnn = MTCNN(image_size=160, margin=0)\n",
    "\n",
    "# Create an inception resnet (in eval mode):\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da8b9",
   "metadata": {},
   "source": [
    "Create Embeddings for Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/test_images/\"\n",
    "database = os.listdir(path)\n",
    "\n",
    "print(\"\\n\\nFollowing persons can be recognised from the given dataset:\")\n",
    "print(database)\n",
    "\n",
    "############################################################################\n",
    "# name_dict = create_dict_with_embeddings(path, database, resnet)\n",
    "##############\n",
    "print(\"\\nEmbeddings are being calculated\", end=\"\")\n",
    "\n",
    "name_dict = {}\n",
    "\n",
    "for person in database:\n",
    "    tensor_list = []\n",
    "    images = os.listdir(path + person)\n",
    "    for image in images:\n",
    "        # Open Image in person-folder\n",
    "        db_img = Image.open(path + person + '/' + image)\n",
    "\n",
    "        # Get cropped and prewhitened image tensor\n",
    "        db_img_cropped = mtcnn(db_img)\n",
    "\n",
    "        # Calculate embedding (unsqueeze to add batch dimension)\n",
    "        db_img_embedding = resnet(db_img_cropped.unsqueeze(0))\n",
    "\n",
    "        # Save embeddings in 'tensor_list'\n",
    "        tensor_list.append(db_img_embedding)\n",
    "    # Save tensor_list for a person in 'name_dict'\n",
    "    name_dict[person] = tensor_list\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "print(\"\\n\\nFinished!\")\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d85565",
   "metadata": {},
   "source": [
    "Create standard embedding from input data and Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af803f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_type = [\"./data/input_data/*.jpg\"]\n",
    "input_images = glob.glob(random.choice(input_path_type))\n",
    "random_image = random.choice(input_images)\n",
    "input_img = Image.open(random_image)\n",
    "\n",
    "crop_path = \"./pytorch_save/img_1.jpg\"\n",
    "\n",
    "# Get cropped and prewhitened image tensor\n",
    "input_img_cropped = mtcnn(input_img, save_path=crop_path)\n",
    "\n",
    "# Calculate embedding (unsqueeze to add batch dimension)\n",
    "input_img_embedding = resnet(input_img_cropped.unsqueeze(0))\n",
    "\n",
    "boxes, probs, landmarks = mtcnn.detect(input_img, landmarks=True)\n",
    "\n",
    "fig, axarray = plt.subplots(1, 2, figsize=(8, 6))\n",
    "axarray[0].imshow(input_img)\n",
    "axarray[0].axis('off')\n",
    "axarray[1].imshow(input_img)\n",
    "axarray[1].axis('off')\n",
    "\n",
    "for box, landmark in zip(boxes, landmarks):\n",
    "    axarray[1].scatter(*np.meshgrid(box[[0, 2]], box[[1, 3]]))\n",
    "    axarray[1].scatter(landmark[:, 0], landmark[:, 1], s=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a1392",
   "metadata": {},
   "source": [
    "Cosine Similarity --> Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fdfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity()\n",
    "best = 0\n",
    "for person in name_dict:\n",
    "    print(person, \": \", end='')\n",
    "    img_index = 1\n",
    "    for tensor in name_dict[person]:\n",
    "        out = cos(tensor, input_img_embedding)\n",
    "        val = out.detach().numpy()[0]\n",
    "        print(val)\n",
    "        if val > best:\n",
    "            best = val\n",
    "            best_list = [person, best, tensor, img_index]\n",
    "            img_index = +1\n",
    "print(\"\\nBest Similarity: \", best_list[0], \",\", best_list[1], \",\", \"#Image:\", best_list[3])\n",
    "\n",
    "threshold = 0.6\n",
    "print(\"\\nValue must remain smaller than threshold = \", threshold)\n",
    "if best_list[1] > threshold:\n",
    "    print(\"\\nPerson is \" + best_list[0])\n",
    "\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(8, 6))\n",
    "    axarr[0].imshow(input_img)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[1].imshow(Image.open('./data/test_images/' + best_list[0] + \"/\" + str(best_list[3]) + \".jpg\"))\n",
    "    axarr[1].axis('off')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"\\nPerson is unknown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb598a9",
   "metadata": {},
   "source": [
    "From here starts the Explainability Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc78869",
   "metadata": {},
   "source": [
    "Rastering the cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90897fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clusteringv15 import cluster_face\n",
    "\n",
    "img = Image.open(crop_path)\n",
    "\n",
    "# clear savepath\n",
    "print(\"Clear savepath.\")\n",
    "chop_path = './chopped_img_temp/'\n",
    "chop_data = os.listdir(chop_path)\n",
    "if chop_data[0] == '.ipynb_checkpoints' and not os.listdir(chop_path):\n",
    "    chop_data.remove('.ipynb_checkpoints')\n",
    "for f in chop_data:\n",
    "    os.remove(os.path.join(chop_path, f))\n",
    "\n",
    "# generate faceclusters and return chops\n",
    "chops = cluster_face(img, denominator=8, savepath=chop_path)\n",
    "\n",
    "chop_data = os.listdir(chop_path)\n",
    "if chop_data[0] == '.ipynb_checkpoints':\n",
    "    chop_data.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c3959",
   "metadata": {},
   "source": [
    "Add noise to Image pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee099e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clusteringv15 import add_noise\n",
    "\n",
    "from cluster_embeddings import cluster_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factors = np.arange(0, 1, 0.1)\n",
    "comp_dict = {}\n",
    "\n",
    "new_comps = cluster_embeddings(img, noise_factors, chop_data, chop_path, chops, mtcnn, resnet)\n",
    "comp_dict.update(new_comps)\n",
    "print(comp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random Images\n",
    "no = 8\n",
    "fig, axarray = plt.subplots(1, no, figsize=(15, 15))\n",
    "noise_path_type = ['./noise_chops/*.jpg']\n",
    "m = 0\n",
    "while m != no:\n",
    "    noise_image = glob.glob(random.choice(noise_path_type))\n",
    "    random_image = random.choice(noise_image)\n",
    "    rndm = Image.open(random_image)\n",
    "    axarray[m].imshow(rndm)\n",
    "    axarray[m].axis('off')\n",
    "    m += 1\n",
    "fig.show()\n",
    "plt.savefig(\"./noise_images.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134a551",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "############################################################################\n",
    "# score = plot_pca(comp_dict, plt)\n",
    "#################\n",
    "score = []\n",
    "pca = PCA()\n",
    "t = 0\n",
    "for comp in comp_dict:\n",
    "    X = comp_dict[comp]\n",
    "\n",
    "    pca.fit(X)\n",
    "    pca_data = pca.transform(X)\n",
    "\n",
    "    per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "    labels = ['PC' + str(x) for x in range(1, len(per_var) + 1)]\n",
    "\n",
    "    plt.bar(x=range(1, len(per_var) + 1), height=per_var, tick_label=labels)\n",
    "    plt.ylabel('Percentage of Explained Variance')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.title(comp)\n",
    "    plt.show()\n",
    "\n",
    "    print(comp + ': ' + str(pca.explained_variance_ratio_[0] * 100))\n",
    "    score.append(pca.explained_variance_ratio_[0])\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff59a0",
   "metadata": {},
   "source": [
    "Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94527451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm as cmx\n",
    "\n",
    "############################################################################\n",
    "# plot_heat_map(chops, score)\n",
    "###############################\n",
    "pic = np.zeros((160, 160))\n",
    "i = 0\n",
    "for clust in chops:\n",
    "    x = clust[0]\n",
    "    while (x - 1) < clust[2]:\n",
    "        y = clust[1]\n",
    "        while (y - 1) < clust[3]:\n",
    "            pic[x, y] = 1 - score[i]\n",
    "            y += 1\n",
    "        x += 1\n",
    "    i += 1\n",
    "cnorm = colors.Normalize(vmin=0, vmax=1)\n",
    "org_img = plt.imshow(Image.open('./pytorch_save/img_1.jpg'))\n",
    "heatmap = plt.imshow(pic, cmap='inferno', norm=cnorm, interpolation='nearest', alpha=0.6)\n",
    "plt.show()\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6e868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
